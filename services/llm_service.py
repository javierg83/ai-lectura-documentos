# services/llm_service.py
"""
Servicio LLM directo (sin chat, sin usuario, sin embeddings).
Usado por extractores sem√°nticos y procesos batch.
"""

from openai import OpenAI
import config


# ==========================================================
# OPENAI CLIENT (MISMO PATR√ìN QUE chat_service.py)
# ==========================================================
oai = OpenAI(api_key=config.API_KEY)


def run_llm_raw(prompt: str) -> str:
    """
    Ejecuta una llamada directa al LLM usando un prompt ya construido.
    Devuelve SOLO texto.
    """
    print("[llm_service] üß† Iniciando llamada LLM (modo batch)")
    print(f"[llm_service] üìè Largo del prompt: {len(prompt)} caracteres")

    try:
        messages = [
            {
                "role": "system",
                "content": (
                    "Eres un asistente experto en an√°lisis de documentos p√∫blicos, "
                    "legales y t√©cnicos. Tu tarea es extraer informaci√≥n estructurada "
                    "de forma precisa, sin inventar datos."
                )
            },
            {
                "role": "user",
                "content": prompt
            }
        ]

        print("[llm_service] üöÄ Enviando solicitud al modelo GPT...")
        resp = oai.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0
        )

        reply = resp.choices[0].message.content
        token_in = resp.usage.prompt_tokens
        token_out = resp.usage.completion_tokens

        print("[llm_service] ‚úÖ Respuesta LLM recibida correctamente")
        print(f"[llm_service] üìä Tokens usados ‚Üí input: {token_in}, output: {token_out}")
        print("[llm_service] üìù Respuesta del modelo (primeros 500 chars):")
        print(reply[:500] + ("..." if len(reply) > 500 else ""))

        return reply.strip()

    except Exception as e:
        print(f"[llm_service] ‚ùå Error al llamar al LLM: {e}")
        raise
