# services/chat_service.py
import json
import redis
import faiss
import numpy as np
import config
from openai import OpenAI
from embeddings import generar_embedding  # ‚úÖ Solo importar lo necesario

redis_client = redis.Redis.from_url(config.REDIS_URL)
oai = OpenAI(api_key=config.API_KEY)

FAISS_INDEX = 'faiss_index.bin'
MAX_CONTEXT_CHARS = 50000


def run_chat(user_id, message, doc_ids, embedding_mode, contenido_manual=None):
    """
    Orquesta el flujo de chat para:
    - contenido_manual: usar texto ya armado (por ejemplo desde chat_embedding).
    - embedding_mode=True: b√∫squeda vectorial en FAISS + chunks en Redis.
    - embedding_mode=False y doc_ids: usar contenido completo del campo 'texto' o 'content' de doc_raw:<doc_id>.
    - sin docs ni contenido_manual: responde solo con el modelo (sin contexto externo).
    """
    print(f"[chat_service] üó£Ô∏è Usuario: {message}")
    system_content = ""

    # 1) Contenido manual (por ejemplo, chunks concatenados desde embedding_service)
    if contenido_manual:
        print("[chat_service] üìÑ Usando contenido manual entregado por Redis.")
        system_content = contenido_manual

    # 2) Modo b√∫squeda vectorial (FAISS) - se mantiene para compatibilidad
    elif embedding_mode and doc_ids:
        print("[chat_service] üîé Modo: b√∫squeda vectorial (FAISS)")
        query_emb = generar_embedding(message)
        if not query_emb:
            print("[chat_service] ‚ùå No se pudo generar embedding para la consulta.")
            return "No se pudo generar la representaci√≥n vectorial de la pregunta.", 0, 0

        try:
            idx = faiss.read_index(FAISS_INDEX)
        except Exception as e:
            print(f"[chat_service] ‚ùå Error al leer √≠ndice FAISS: {e}")
            return "No se pudo acceder al √≠ndice vectorial para buscar informaci√≥n.", 0, 0

        D, I = idx.search(np.array([np.array(query_emb, dtype='float32')]), 10)
        print(f"[chat_service] ‚Üí Comparando contra {len(I[0])} chunks FAISS")

        seen = set()
        chunks_agregados = 0

        for doc_id in doc_ids:
            for i in I[0]:
                if i < 0:
                    continue
                if (doc_id, int(i)) in seen:
                    continue
                seen.add((doc_id, int(i)))

                txt = redis_client.hget(f"chunk:{doc_id}:{int(i)}", 'text')
                if txt:
                    decoded = txt.decode('utf-8', errors='ignore')
                    system_content += decoded + "\n\n"
                    chunks_agregados += 1
                    print(f"[chat_service] üîπ Chunk {int(i)} incluido del doc {doc_id}:")
                    print(decoded[:200] + "...\n")
                if chunks_agregados >= 5:
                    break
            if chunks_agregados >= 5:
                break

        if chunks_agregados == 0:
            print("[chat_service] ‚ö†Ô∏è No se encontraron chunks relevantes en Redis.")

    # 3) Modo sin embeddings: usar contenido completo por documento
    elif doc_ids:
        print("[chat_service] üìü Modo: contenido completo por documento")
        for doc_id in doc_ids:
            rec = redis_client.hgetall(f"doc_raw:{doc_id}")
            if not rec:
                print(f"[chat_service] ‚ö†Ô∏è No se encontr√≥ contenido en Redis para doc_raw:{doc_id}")
                continue

            # ‚úÖ Preferir campo 'texto' (lo que t√∫ usas), con fallback a 'content'
            raw_bytes = rec.get(b'texto') or rec.get(b'content') or b''
            content = raw_bytes.decode('utf-8', errors='ignore')
            print(f"[chat_service] ‚Üí Texto cargado (len={len(content)})")

            if len(content) > MAX_CONTEXT_CHARS:
                print(f"[chat_service] ‚ö†Ô∏è Documento demasiado largo (len={len(content)}). Usando chunks como respaldo.")
                for i in range(10):
                    chunk = redis_client.hget(f"chunk:{doc_id}:{i}", 'text')
                    if chunk:
                        system_content += chunk.decode('utf-8', errors='ignore') + "\n\n"
            else:
                print(f"[chat_service] ‚Üí Usando texto de doc_raw:{doc_id} (len={len(content)})")
                system_content += content + "\n\n"

    # 4) Sin contexto externo: solo conversaci√≥n libre
    else:
        print("[chat_service] üí¨ Modo sin documentos asociados (conversaci√≥n general).")
        system_content = ""

    if not system_content.strip():
        print("[chat_service] ‚ùå No se pudo construir contexto para enviar al modelo.")
        return "No se obtuvo contexto relevante para responder la consulta.", 0, 0

    prompt_base = (
        "Responde siempre en espa√±ol. "
        "Eres un asistente experto en an√°lisis de documentos p√∫blicos, legales y t√©cnicos. "
        "Tu tarea es ayudar al usuario a entender los documentos proporcionados, "
        "utilizando **solo** la informaci√≥n que estos contienen.\n\n"
        "### Instrucciones estrictas:\n"
        "- No puedes inventar datos que no est√©n expl√≠citamente en el documento.\n"
        "- Usa solo el contenido que te proporcionamos como sistema (texto del/los documentos).\n"
        "- Si no encuentras informaci√≥n suficiente, responde claramente que no se encontr√≥ esa informaci√≥n.\n"
        "- Si el contenido proviene de una tabla, extrae su contenido y pres√©ntalo en formato claro para el usuario.\n"
        "- Siempre que sea posible, cita o referencia el nombre del archivo o secci√≥n del documento que utilizas.\n"
        "- Si el usuario pide un resumen, entr√©galo de forma ordenada y f√°cil de leer (t√≠tulos, vi√±etas, etc.).\n"
        "- Aseg√∫rate de que cada respuesta sea √∫til, clara y avance en la conversaci√≥n.\n"
        "- Copia y pega el parrafo en caso del que usuario te lo solicite.\n"
    )

    history = [{'role': 'system', 'content': prompt_base}]
    if system_content.strip():
        history.append({'role': 'system', 'content': system_content})
    history.append({'role': 'user', 'content': message})

    try:
        print("[chat_service] üß† Enviando solicitud al modelo GPT...")
        resp = oai.chat.completions.create(
            model="gpt-4o-mini",
            messages=history
        )

        reply = resp.choices[0].message.content
        token_in = resp.usage.prompt_tokens
        token_out = resp.usage.completion_tokens

        print(f"[chat_service] ‚úÖ Respuesta generada con √©xito")
        print(f"[chat_service] üìä Tokens usados ‚Üí input: {token_in}, output: {token_out}")
        print(f"[chat_service] üìù Respuesta del modelo:")
        print(reply)

        redis_client.rpush(
            f"chat_history:{user_id}",
            json.dumps({'role': 'assistant', 'content': reply}, ensure_ascii=False)
        )
        print("[chat_service] üì§ Payload enviado al frontend: {'role': 'assistant', 'content': ...}")
        return reply, token_in, token_out

    except Exception as e:
        print(f"[chat_service] ‚ùå Error al llamar a OpenAI: {e}")
        return "Ocurri√≥ un error al procesar la consulta con el modelo de lenguaje.", 0, 0
